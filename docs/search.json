[
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "M7550: Final Project",
    "section": "Project Overview",
    "text": "Project Overview\nIn our project, we implement systematic prediction methodologies for analyzing student utilization patterns within the BGSU Learning Commons (LC). Through rigorous statistical learning approaches, we address two distinct prediction challenges: visit duration estimation and occupancy forecasting. The implementation leverages specialized modeling architectures to capture the unique characteristics of each prediction task."
  },
  {
    "objectID": "index.html#data-architecture",
    "href": "index.html#data-architecture",
    "title": "M7550: Final Project",
    "section": "Data Architecture",
    "text": "Data Architecture\nThe research framework employs a comprehensive dataset spanning two academic years. The training corpus encompasses Fall 2016 through Spring 2017, providing the foundation for model development and parameter optimization. Our validation framework utilizes subsequent academic year data (Fall 2017 - Spring 2018) to assess model generalization and stability. The feature space integrates demographic indicators, academic metrics, and temporal patterns, while accounting for an observed senior-class representation bias in the underlying data collection process."
  },
  {
    "objectID": "index.html#methodological-framework",
    "href": "index.html#methodological-framework",
    "title": "M7550: Final Project",
    "section": "Methodological Framework",
    "text": "Methodological Framework\nOur preprocessing architecture implements systematic feature engineering across multiple domains. The temporal component decomposes visit patterns into hierarchical time scales, capturing daily rhythms, weekly cycles, and semester-long trends. Academic context modeling integrates course-level hierarchies with performance metrics, while our statistical standardization protocol employs RobustScaler methodology for outlier resilience.\nThe modeling architecture employs task-specific approaches while maintaining shared foundational components. The core implementation utilizes PenalizedSplines as the primary architecture, supplemented by Ridge and Lasso regression variants for linear modeling and KNN for local pattern capture. Duration-specific modeling incorporates Penalized Log-Normal GLM for pattern modeling, while occupancy prediction employs specialized Poisson and Weibull architectures for count-based forecasting."
  },
  {
    "objectID": "index.html#implementation-results",
    "href": "index.html#implementation-results",
    "title": "M7550: Final Project",
    "section": "Implementation Results",
    "text": "Implementation Results\nThe duration prediction framework achieves an RMSE of 59.47 minutes with an R² of 0.059, utilizing PenalizedSplines with optimized parameters (Ridge α: 14.38, spline degree: 3, knot count: 15).\nThe occupancy prediction system demonstrates enhanced predictive capacity with an RMSE of 3.64 students and R² of 0.303, employing similar architectural components with task-specific parameter optimization."
  },
  {
    "objectID": "index.html#technical-infrastructure",
    "href": "index.html#technical-infrastructure",
    "title": "M7550: Final Project",
    "section": "Technical Infrastructure",
    "text": "Technical Infrastructure\nOur implementation leverages the Python scientific computing ecosystem, with scikit-learn providing the core modeling framework and MLflow enabling systematic experiment tracking. The data processing pipeline integrates pandas and numpy for efficient computation, while visualization capabilities combine matplotlib, seaborn, and plotly for comprehensive analysis. The training infrastructure implements automated parameter optimization through grid search methodology, while maintaining systematic version control through MLflow’s tracking capabilities."
  },
  {
    "objectID": "index.html#research-findings",
    "href": "index.html#research-findings",
    "title": "M7550: Final Project",
    "section": "Research Findings",
    "text": "Research Findings\nDuration prediction presents significant challenges due to high variance in visit patterns and right-skewed distributions. The limited predictive capacity (R² = 0.059) suggests underlying complexity in individual visit duration behaviors. In contrast, occupancy prediction demonstrates more robust performance (R² = 0.303), successfully capturing usage patterns and providing reliable concurrent usage estimates."
  },
  {
    "objectID": "index.html#future-research-directions",
    "href": "index.html#future-research-directions",
    "title": "M7550: Final Project",
    "section": "Future Research Directions",
    "text": "Future Research Directions\nThis research framework enables several promising extensions. Environmental factor integration, particularly weather patterns, could enhance predictive capacity. Deep pattern analysis through advanced non-linear methodologies and specialized time series approaches warrant investigation. Additionally, ensemble architectures combining multiple modeling paradigms present opportunities for performance enhancement.\nImplementation details and comprehensive analysis available in associated documentation.\n\nsource: src/index.clj"
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#temporal-feature-engineering",
    "href": "notebooks/report/feature_engineering.html#temporal-feature-engineering",
    "title": "Feature Engineering",
    "section": "Temporal Feature Engineering",
    "text": "Temporal Feature Engineering\nOur feature engineering process began with temporal data extraction using the lubridate package. The timestamp data provided several readily constructible features:\nprepare_dates &lt;- function(df) {\n df %&gt;% mutate(\n   Check_In_Date = mdy(Check_In_Date),\n   Check_In_Time = hms::as_hms(Check_In_Time)\n )\n}\nFrom these validated timestamps, we construct several temporal features:\nadd_temporal_features &lt;- function(df) {\n df %&gt;% mutate(\n    Check_In_Day = wday(Check_In_Date, label = TRUE),\n    Is_Weekend = Check_In_Day %in% c(\"Sat\", \"Sun\"),\n    Check_In_Week = ceiling(day(Check_In_Date) / 7),\n    Check_In_Month = month(Check_In_Date, label = TRUE),\n    Check_In_Hour = hour(Check_In_Time)\n )\n}\nAnalysis of visit patterns revealed a non-linear relationship between Check_In_Hour and Duration variables. This observation prompted the creation of a more nuanced Time_Category variable with distinct periods:\nadd_time_category &lt;- function(df) {\n df %&gt;% mutate(\n   Time_Category = case_when(\n       hour(Check_In_Time) &lt; 6 ~ \"Late Night\",\n       hour(Check_In_Time) &lt; 12 ~ \"Morning\",\n       hour(Check_In_Time) &lt; 17 ~ \"Afternoon\",\n       hour(Check_In_Time) &lt; 22 ~ \"Evening\",\n       TRUE ~ \"Late Night\"\n   )\n )\n}\nThe Expected_Graduation variable presented a dimensionality challenge due to its categorical semester format. We addressed this by converting it to a numeric ‘Months_Until_Graduation’ metric, effectively reducing complexity while maintaining predictive potential.\nconvert_semester_to_date &lt;- function(semester_str) {\n   parts &lt;- strsplit(semester_str, \" \")[[1]]\n   year &lt;- parts[length(parts)]\n   semester &lt;- parts[1]\n   month &lt;- case_when(\n      semester == \"Fall\" ~ \"08\",\n      semester == \"Spring\" ~ \"01\",\n      semester == \"Summer\" ~ \"06\",\n      semester == \"Winter\" ~ \"12\",\n      TRUE ~ NA_character_\n   )\n   paste0(month, \"/\", \"01\", \"/\", year)\n}\nadd_graduation_features &lt;- function(df) {\n   df %&gt;% mutate(\n      Months_Until_Graduation = as.numeric(\n         difftime(Expected_Graduation_Date, Semester_Date, units = \"days\") / 30.44\n      )\n   )\n}"
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#course-related-features",
    "href": "notebooks/report/feature_engineering.html#course-related-features",
    "title": "Feature Engineering",
    "section": "Course-Related Features",
    "text": "Course-Related Features\nThe Course_Code_by_Thousands variable exhibited significant ambiguity when treated categorically. Our analysis indicated the need for a more structured approach, leading to the following classification systems:\nadd_course_related_features &lt;- function(df) {\n  df %&gt;% mutate(\n    Course_Level = case_when(\n      Course_Code_by_Thousands &lt;= 100 ~ \"Special\",\n      Course_Code_by_Thousands &lt;= 3000 ~ \"Lower Classmen\",\n      Course_Code_by_Thousands &lt;= 4000 ~ \"Upper Classmen\",\n      Course_Code_by_Thousands &gt; 4000 ~ \"Graduate\"\n    )\n  )\n}\nTo capture academic performance context, we developed categorical features for GPA and Credit_Load:\nadd_academic_performance_features &lt;- function(df) {\n  df %&gt;% mutate(\n    GPA_Category = case_when(\n      GPA &gt;= 3.5 ~ \"Excellent\",\n      GPA &gt;= 3.0 ~ \"Good\",\n      GPA &gt;= 2.0 ~ \"Satisfactory\",\n      TRUE ~ \"Needs Improvement\"\n    )\n  )\n}\nadd_credit_load_features &lt;- function(df) {\n  df %&gt;% mutate(\n   Credit_Load_Category = case_when(\n     Credit_Load &lt;= 6 ~ \"Part Time\",\n     Credit_Load &lt;= 12 ~ \"Half Time\",\n     Credit_Load &lt;= 18 ~ \"Full Time\",\n     TRUE ~ \"Overload\"\n    )\n  )\n}"
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#student-classification-features",
    "href": "notebooks/report/feature_engineering.html#student-classification-features",
    "title": "Feature Engineering",
    "section": "Student Classification Features",
    "text": "Student Classification Features\nThe dataset exhibited an unexpected concentration of ‘Senior’ classifications in our initial analysis. Further investigation revealed this stemmed from students accumulating excess credits for senior status without fulfilling graduation requirements. To address this imbalance while preserving useful information, we implemented a dual classification approach.\nadd_class_standing_category &lt;- function(df) {\n  df %&gt;% mutate(\n    # Renaming column and values for Class_Standing\n    Class_Standing_Self_Reported = case_when(\n      Class_Standing == \"Freshman\" ~ \"First Year\",\n      Class_Standing == \"Sophomore\" ~ \"Second Year\",\n      Class_Standing == \"Junior\" ~ \"Third Year\",\n      Class_Standing == \"Senior\" ~ \"Fourth Year\",\n      TRUE ~ Class_Standing\n    ),\n  )\n}\nThe original Class_Standing variable, while potentially containing valuable self-reported insights, required recoding. We preserved this information as Class_Standing_Self_Reported with progression labels from “First Year” through “Fourth Year”, along with “Graduate” and “Other” designations. Complementing this, we developed a more objective BGSU Standing metric based on credit hours. This dual approach preserves potentially valuable self-reported information while introducing a more objective credit-based metric.\nadd_class_standing_bgsu &lt;- function(df) {\n  df %&gt;% mutate(\n    # Class_standing by BGSU's definition\n    # https://www.bgsu.edu/academic-advising/student-resources/academic-standing.html\n    Class_Standing_BGSU = case_when(\n      Total_Credit_Hours_Earned &lt; 30 ~ \"Freshman\",\n      Total_Credit_Hours_Earned &lt; 60 ~ \"Sophomore\",\n      Total_Credit_Hours_Earned &lt; 90 ~ \"Junior\",\n      Total_Credit_Hours_Earned &lt;= 120 ~ \"Senior\",\n      TRUE ~ \"Extended\"\n    ),\n  )\n}"
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#course-name-and-type-features",
    "href": "notebooks/report/feature_engineering.html#course-name-and-type-features",
    "title": "Feature Engineering",
    "section": "Course Name and Type Features",
    "text": "Course Name and Type Features\nThe Course_Name variable presented immediate challenges for model fitting in its raw form. While various approaches existed for handling this high-cardinality variable, we opted for a flexible keyword-based system. This approach identifies key terms within course names - for instance, classifying courses containing ‘Culture’, ‘Language’, or ‘Ethics’ under ‘Humanities’. Though this resulted in 14 distinct categories, it provides flexibility for subsequent modeling decisions through regularization or variable selection.\nadd_course_name_category &lt;- function(df) {\n  df %&gt;% mutate(\n    Course_Name_Category = case_when(\n      # Introductory level courses\n      grepl(\"Algebra|Basic|Elementary|Intro|Introduction|Fundamental|General|Principles|Orientation\", \n            Course_Name, ignore.case = TRUE) ~ \"Introductory\",\n      \n      # Intermediate level courses\n      grepl(\"Intermediate|II$|II |2|Applied\", \n            Course_Name, ignore.case = TRUE) ~ \"Intermediate\",\n      \n      # Advanced level courses\n      grepl(\"Advanced|III|3|Analysis|Senior|Graduate|Dissertation|Research|Capstone\", Course_Name, ignore.case = TRUE) ~ \"Advanced\",\n      \n      # Business related courses\n      grepl(\"Business|Finance|Accounting|Economics|Marketing|Management\", \n            Course_Name, ignore.case = TRUE) ~ \"Business\",\n      \n      # Laboratory/Practical courses\n      grepl(\"Laboratory|Lab\", Course_Name, ignore.case = TRUE) ~ \"Laboratory\",\n      \n      # Seminar/Workshop courses\n      grepl(\"Seminar|Workshop\", Course_Name, ignore.case = TRUE) ~ \"Seminar\",\n      \n      # Independent/Special courses\n      grepl(\"Independent|Special\", Course_Name, ignore.case = TRUE) ~ \"Independent Study\",\n      \n      # Mathematics and Statistics\n      grepl(\"Mathematics|Calculus|Statistics|Probability|Geometry|Discrete\", \n            Course_Name, ignore.case = TRUE) ~ \"Mathematics\",\n      \n      # Computer Science\n      grepl(\"Computer|Programming|Data|Software|Network|Database|Algorithm\", \n            Course_Name, ignore.case = TRUE) ~ \"Computer Science\",\n      \n      # Natural Sciences\n      grepl(\"Physics|Chemistry|Biology|Astronomy|Earth|Environment|Science\", \n            Course_Name, ignore.case = TRUE) ~ \"Natural Sciences\",\n      \n      # Social Sciences\n      grepl(\"Psychology|Sociology|Anthropology|Social|Cultural|Society\", \n            Course_Name, ignore.case = TRUE) ~ \"Social Sciences\",\n      \n      # Humanities\n      grepl(\"History|Philosophy|Ethics|Literature|Culture|Language|Art\", \n            Course_Name, ignore.case = TRUE) ~ \"Humanities\",\n      \n      # Education/Teaching\n      grepl(\"Education|Teaching|Learning|Childhood|Teacher|Curriculum\", \n            Course_Name, ignore.case = TRUE) ~ \"Education\",\n      \n      # Default case\n      TRUE ~ \"Other\"\n    )\n  )\n}\nSimilarly, the Course_Type variable required substantial level reduction. We consolidated the original categories into natural academic groupings such as ‘business courses’, ‘education courses’, and ‘STEM courses’. For visits lacking course specifications, we designated a “No Response” category rather than discarding these observations.\nadd_course_type_category &lt;- function(df) {\n  df %&gt;% mutate(\n    Course_Type_Category = case_when(\n      # STEM Fields\n      Course_Type %in% c(\"MATH\", \"STAT\", \"CS\", \"ASTR\",\"PHYS\", \"BIOL\", \"CHEM\", \"GEOL\", \"ECET\") ~ \"STEM Core\",\n      \n      # Engineering and Technology\n      Course_Type %in% c(\"ENGT\", \"CONS\", \"ARCH\", \"MIS\", \"TECH\") ~ \"Engineering & Technology\",\n      \n      # Business and Economics\n      Course_Type %in% c(\"FIN\", \"ACCT\", \"ECON\", \"BA\", \"MGMT\", \"MKT\", \"MBA\", \"BIZX\", \"LEGS\", \"OR\") ~ \"Business\",\n      \n      # Social Sciences\n      Course_Type %in% c(\"SOC\", \"PSYC\", \"POLS\", \"CRJU\", \"HDFS\", \"SOWK\", \"GERO\") ~ \"Social Sciences\",\n      \n      # Natural and Health Sciences\n      Course_Type %in% c(\"NURS\", \"MLS\", \"EXSC\", \"FN\", \"AHTH\", \"DHS\") ~ \"Health Sciences\",\n      \n      # Humanities and Languages\n      Course_Type %in% c(\"HIST\", \"PHIL\", \"ENG\", \"GSW\", \"FREN\", \"GERM\", \"SPAN\", \"LAT\", \"RUSN\", \"ITAL\", \"CLCV\") ~ \"Humanities\",\n      \n      # Arts and Performance\n      Course_Type %in% c(\"ART\", \"ID\", \"MUCT\", \"MUS\", \"THFM\", \"POPC\") ~ \"Arts\",\n      \n      # Education and Teaching\n      Course_Type %in% c(\"EDTL\", \"EDFI\", \"EDIS\", \"EIEC\") ~ \"Education\",\n      \n      # Environmental Studies\n      Course_Type %in% c(\"ENVS\", \"GEOG\", \"SEES\") ~ \"Environmental Studies\",\n      \n      # Special Programs\n      Course_Type %in% c(\"HNRS\", \"UNIV\", \"ORGD\", \"RESC\") ~ \"Special Programs\",\n      \n      # Physical Education\n      Course_Type %in% c(\"PEG\", \"SM\", \"HMSL\") ~ \"Physical Education\",\n      \n      # Cultural Studies\n      Course_Type %in% c(\"ETHN\", \"COMM\", \"CDIS\") ~ \"Cultural & Communication Studies\",\n      \n      # No Response/Unknown\n      Course_Type %in% c(\"No Response\", NA) ~ \"No Response\",\n      \n      # Default case\n      TRUE ~ \"Other\"\n    )\n  )\n}\nFor visits without a specified course association, we introduced a “No Response” category to maintain data completeness."
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#major-categories",
    "href": "notebooks/report/feature_engineering.html#major-categories",
    "title": "Feature Engineering",
    "section": "Major Categories",
    "text": "Major Categories\nThe Major variable demanded a similar keyword-based reduction strategy as Course_Name. Through analysis of major descriptions, we identified recurring terms that allowed for logical grouping. For example, the ‘Mathematics’ category encompasses mathematics, statistics, and actuarial science majors. Our final categorization includes:\nadd_major_category &lt;- function(df) {\n  df %&gt;% mutate(\n    Major_Category = case_when(\n      # Business and Management\n      grepl(\"MBA|BSBA|Business|Marketing|Finance|Account|Economics|Management|Supply Chain|Analytics\", \n            Major, ignore.case = TRUE) ~ \"Business\",\n      \n      # Computer Science and Technology\n      grepl(\"Computer|Software|Data|Information Systems|Technology|Engineering|Electronics\", \n            Major, ignore.case = TRUE) ~ \"Computing & Technology\",\n      \n      # Natural Sciences\n      grepl(\"Biology|Chemistry|Physics|Science|Environmental|Geology|Forensic|Neuroscience\", \n            Major, ignore.case = TRUE) ~ \"Natural Sciences\",\n      \n      # Health Sciences\n      grepl(\"Nursing|Health|Medical|Nutrition|Dietetics|Physical Therapy|Physician|Laboratory\", \n            Major, ignore.case = TRUE) ~ \"Health Sciences\",\n      \n      # Social Sciences\n      grepl(\"Psychology|Sociology|Criminal Justice|Political|Economics|Social Work|Anthropology\", \n            Major, ignore.case = TRUE) ~ \"Social Sciences\",\n      \n      # Education\n      grepl(\"Education|Teaching|Early Childhood|BSED|Intervention Specialist\", \n            Major, ignore.case = TRUE) ~ \"Education\",\n      \n      # Arts and Humanities\n      grepl(\"Art|Music|Philosophy|History|English|Language|Communication|Media|Journalism|Film|Theatre\", \n            Major, ignore.case = TRUE) ~ \"Arts & Humanities\",\n      \n      # Mathematics and Statistics\n      grepl(\"Math|Statistics|Actuarial\", \n            Major, ignore.case = TRUE) ~ \"Mathematics\",\n      \n      # Pre-Professional Programs\n      grepl(\"Pre-|PRELAW|PREMED|PREVET\", \n            Major, ignore.case = TRUE) ~ \"Pre-Professional\",\n      \n      # Undecided/General Studies\n      grepl(\"Undecided|Liberal Studies|General|Deciding|UND|Individual|BLS\", \n            Major, ignore.case = TRUE) ~ \"General Studies\",\n      \n      # Special Programs\n      grepl(\"Minor|Certificate|GCERT|Non-Degree\", \n            Major, ignore.case = TRUE) ~ \"Special Programs\",\n      \n      # No Response/Unknown\n      grepl(\"No Response|NA\", Major, ignore.case = TRUE) ~ \"No Response\",\n      \n      # Default case\n      TRUE ~ \"Other\"\n    ),\n    \n    # Add a flag for double majors\n    Has_Multiple_Majors = grepl(\",\", Major)\n  )\n}\nWe maintained an ‘Other’ category for majors that defied clear classification. The data structure also revealed an opportunity to identify students pursuing multiple degrees - we created this indicator by detecting comma-separated entries in the Major field."
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#visit-pattern-features",
    "href": "notebooks/report/feature_engineering.html#visit-pattern-features",
    "title": "Feature Engineering",
    "section": "Visit Pattern Features",
    "text": "Visit Pattern Features\nStudent_ID analysis enabled the construction of several usage metrics. Beyond simple visit counts, we examined temporal patterns at multiple scales:\nadd_visit_features &lt;- function(df) {\n  df %&gt;%\n    group_by(Student_IDs) %&gt;%\n    mutate(\n      # Count visits per student\n      Total_Visits = n(),\n      # Count visits per student per semester\n      Semester_Visits = n_distinct(Check_In_Date),\n      # Average visits per week\n      Avg_Weekly_Visits = Semester_Visits / max(Semester_Week)\n    ) %&gt;%\n    ungroup()\n}\nadd_week_volume_category &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      Week_Volume = case_when(\n        Semester_Week %in% c(4:8, 10:13, 15:16) ~ \"High Volume\",\n        Semester_Week %in% c(1:3, 9, 14, 17) ~ \"Low Volume\",\n        TRUE ~ \"Other\"\n      )\n    )\n}\nExamination of visit frequency throughout the semester revealed clear patterns. Weeks 1-3, 9, 14, and 17 consistently showed lower activity levels, while the remaining weeks demonstrated higher traffic. This distinction proved valuable, as visit volume may influence individual visit duration. We encoded this insight through a binary ‘Volume’ indicator for each week.\n\n\n\nWeek Visits"
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#course-load-and-performance-features",
    "href": "notebooks/report/feature_engineering.html#course-load-and-performance-features",
    "title": "Feature Engineering",
    "section": "Course Load and Performance Features",
    "text": "Course Load and Performance Features\nFor each student-semester combination, we developed metrics to capture academic context. We tracked the number of unique courses and examined the distribution of course levels based on the Course_Code_by_Thousands variable. Particular attention was paid to upper-division coursework, creating a specific metric for the proportion of ‘4000-level courses’. Additionally, we implemented a GPA trend indicator that focuses on directional changes rather than absolute values, recognizing that the direction of GPA movement might be more informative than the magnitude.\nadd_course_load_features &lt;- function(df) {\n  df %&gt;%\n    group_by(Student_IDs, Semester) %&gt;%\n    mutate(\n      # Number of unique courses\n      Unique_Courses = n_distinct(Course_Number),\n      # Mix of course levels\n      Course_Level_Mix = n_distinct(Course_Code_by_Thousands),\n      # Proportion of advanced courses\n      Advanced_Course_Ratio = mean(Course_Level == \"Upper Classmen\", na.rm = TRUE)\n    ) %&gt;%\n    ungroup()\n}\nadd_gpa_trend &lt;- function(df) {\n  df %&gt;% mutate(\n    # Calculate GPA trend (1 for positive, -1 for negative, 0 for no change)\n    GPA_Trend = sign(Change_in_GPA),\n  )\n}"
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#group-dynamics",
    "href": "notebooks/report/feature_engineering.html#group-dynamics",
    "title": "Feature Engineering",
    "section": "Group Dynamics",
    "text": "Group Dynamics\nA final analytical step involved identifying group study patterns. By examining clusters of Check_In_Time, we detected multiple students arriving within the same minute - a strong indicator of group visits. This observation led to three complementary features: Group_Size, Group_Check_In, and Group_Size_Category.\nadd_group_features &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      Check_In_Timestamp = ymd_hms(paste(Check_In_Date, Check_In_Time))\n    ) %&gt;%\n    add_count(Check_In_Timestamp, name = \"Group_Size\") %&gt;%\n    mutate(\n      Group_Check_In = Group_Size &gt; 1,\n      Group_Size_Category = case_when(\n        Group_Size == 1 ~ \"Individual\",\n        Group_Size &lt;= 3 ~ \"Small Group\",\n        Group_Size &lt;= 6 ~ \"Medium Group\",\n        TRUE ~ \"Large Group\"\n      )\n    ) %&gt;%\n    select(-Check_In_Timestamp)\n}\nWhile some simultaneous check-ins might be coincidental, this classification captures potential social patterns in Learning Commons usage, particularly among friend groups."
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#data-quality-of-duration-and-occupancy",
    "href": "notebooks/report/feature_engineering.html#data-quality-of-duration-and-occupancy",
    "title": "Feature Engineering",
    "section": "Data Quality of Duration and Occupancy",
    "text": "Data Quality of Duration and Occupancy\nLast, our preprocessing included essential validation steps. We verified Duration_In_Min calculations through comparison of check-in and check-out times, ensuring no negative values existed in the data. The Occupancy variable was calculated and received similar scrutiny during its construction.\nensure_duration &lt;- function(df) {\n  # Calculate duration in minutes\n  df %&gt;%\n    mutate(\n      Duration_In_Min = as.numeric(difftime(\n        Check_Out_Time,\n        Check_In_Time,\n        units = \"mins\"\n      )),\n      # Filter out negative durations\n      Duration_In_Min = if_else(Duration_In_Min &lt; 0, NA_real_, Duration_In_Min),\n    ) %&gt;%\n    filter(!is.na(Duration_In_Min))\n}\ncalculate_occupancy &lt;- function(df) {\n  df %&gt;%\n    arrange(Check_In_Date, Check_In_Time) %&gt;%\n    group_by(Check_In_Date) %&gt;%\n    mutate(\n      Cum_Arrivals = row_number(),\n      Cum_Departures = sapply(seq_along(Check_In_Time), function(i) {\n        sum(!is.na(Check_Out_Time[1:i]) & \n            Check_Out_Time[1:i] &lt;= Check_In_Time[i])\n      }),\n      Occupancy = Cum_Arrivals - Cum_Departures\n    ) %&gt;%\n    select(-c(Cum_Arrivals, Cum_Departures))\n}"
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#conclusion",
    "href": "notebooks/report/feature_engineering.html#conclusion",
    "title": "Feature Engineering",
    "section": "Conclusion",
    "text": "Conclusion\nOur feature engineering process addressed several key challenges in the Learning Commons dataset through systematic transformation and enrichment of the raw data. The temporal features capture both cyclical patterns and academic calendar effects, while our treatment of course-related variables reduces dimensionality while preserving meaningful distinctions. The dual approach to student classification acknowledges both institutional definitions and self-reported status, providing complementary perspectives on academic progression.\nThe keyword-based categorization systems for Course_Name, Course_Type, and Major strike a balance between granularity and model practicality. While some nuance is inevitably lost in such consolidation, the resulting features maintain interpretability while reducing sparsity. The visit pattern features capture both individual usage trends and broader facility utilization patterns, providing context for duration prediction.\nOur treatment of group dynamics represents a novel approach to capturing social patterns in academic space utilization. While the simultaneous check-in heuristic may occasionally misclassify coincidental arrivals, it provides valuable insight into collaborative learning patterns that might influence visit duration.\nThe extensive validation steps for Duration_In_Min and Occupancy calculations ensure data quality while acknowledging practical limitations. These features form a robust foundation for subsequent modeling efforts, though opportunities exist for further refinement through domain expert consultation and iterative testing."
  },
  {
    "objectID": "notebooks/report/models_overview.html#framework-architecture",
    "href": "notebooks/report/models_overview.html#framework-architecture",
    "title": "Models & Pipelines",
    "section": "Framework Architecture",
    "text": "Framework Architecture\nThe modeling framework implements prediction pipelines that operate on the engineered features described in the previous chapter. The implementation resides in the src/python/models directory: algorithms_duration.py, algorithms_occupancy.py, pipelines.py, and cross_validation.py."
  },
  {
    "objectID": "notebooks/report/models_overview.html#cross-validation-methodology",
    "href": "notebooks/report/models_overview.html#cross-validation-methodology",
    "title": "Models & Pipelines",
    "section": "Cross-Validation Methodology",
    "text": "Cross-Validation Methodology\nOur prediction framework operates on two complementary datasets: LC_train, containing both features and response variables, and LC_test, containing only features. The validation process begins with a strategic partition of LC_train into training and holdout segments, allocating 80% and 20% of the data respectively.\nThe training segment serves as the foundation for model development through cross-validation. During this phase, we systematically evaluate different model architectures, pipeline configurations, and hyperparameter combinations. Each candidate model undergoes rigorous testing across multiple data splits, allowing us to assess its stability and predictive power under varying conditions.\nThe holdout segment provides validation of our model choices. By evaluating performance on this previously unseen data, we can detect potential overfitting and ensure our model generalizes effectively beyond its training examples. This validation guides our selection of the optimal model configuration, including the choice between architectures like Ridge or Lasso regression, pipeline variants such as vanilla or interaction-based approaches, and appropriate cross-validation strategies.\nAfter identifying the strongest configuration through this validation process, we proceed to train our production model. This final training phase utilizes the complete LC_train dataset, incorporating all available labeled data to maximize the model’s predictive capabilities. The resulting model can then generate predictions for the unlabeled LC_test data with confidence grounded in our thorough validation methodology.\n\n\n\nCross-Validation"
  },
  {
    "objectID": "notebooks/report/models_overview.html#core-components",
    "href": "notebooks/report/models_overview.html#core-components",
    "title": "Models & Pipelines",
    "section": "Core Components",
    "text": "Core Components\nTo implement this validation methodology effectively, our framework relies on three essential elements: base algorithms that handle the core prediction tasks, feature processing pipelines that transform raw data into meaningful inputs, and cross-validation strategies that ensure reliable performance assessment. We will start with the cross-validation framework because we discussed the purpose directly above.\n\nCross-Validation Framework\nThe framework implements three validation strategies:\ndef get_cv_methods(n_samples: int):\n    n_splits = 10\n    default_test_size = n_samples // (n_splits + 1)\n\n    return {\n        'kfold': KFold(\n            n_splits=10, \n            shuffle=True, \n            random_state=3\n        ),\n        'rolling': TimeSeriesSplit(\n            n_splits=n_splits,\n            max_train_size=default_test_size * 5,\n            test_size=default_test_size\n        ),\n        'expanding': TimeSeriesSplit(\n            n_splits=n_splits,\n            test_size=default_test_size\n        )\n    }\n\n\n\n\n\n\n\n\n\nStrategy\nDescription\nCharacteristics\nBest For\n\n\n\n\nkfold\nRandom k splits\n- Provides baseline performance- Less suitable for temporal patterns\nDuration prediction\n\n\nrolling\nFixed-size moving window\n- Captures recent temporal dependencies- Maintains consistent training size\nOccupancy prediction\n\n\nexpanding\nGrowing window\n- Accumulates historical data- Increases training size over time- Balances temporal and volume effects\nLong-term trends\n\n\n\n\n\nAlgorithm Architecture\nThe duration prediction models include:\ndef get_model_definitions():\n    return {\n        'Ridge': (Ridge(), {\n            'model__alpha': np.logspace(0, 2, 10),\n            'select_features__k': np.arange(10, 55, 5),\n        }),\n        'Lasso': (Lasso(), {\n            'model__alpha': np.logspace(-2, 0, 10),\n            'select_features__k': np.arange(10, 55, 5),\n        }),\n        'PenalizedSplines': (Pipeline([\n            ('spline', SplineTransformer()),\n            ('ridge', Ridge())\n        ]), {\n            'model__spline__n_knots': [9, 11, 13, 15],\n            'model__spline__degree': [3],\n            'model__ridge__alpha': np.logspace(0, 2, 20),\n            'select_features__k': np.arange(10, 55, 5),\n        }),\n        'KNN': (KNeighborsRegressor(), {\n            'model__n_neighbors': np.arange(15, 22, 2),\n            'model__weights': ['uniform', 'distance'],\n            'select_features__k': np.arange(10, 55, 5),\n        }),\n    }\n\n\n\n\n\n\n\n\nModel Type\nKey Characteristics\nHyperparameter Range\n\n\n\n\nRidge\nLinear with L2 penalty\n\\(\\alpha \\in [10^0, 10^2]\\)\n\n\nLasso\nLinear with L1 penalty\n\\(\\alpha \\in [10^{-2}, 10^0]\\)\n\n\nPenalizedSplines\nCubic splines with ridge penalty\nknots: {9, 11, 13, 15},  ridge: \\(\\alpha \\in [10^0, 10^2]\\)\n\n\nKNN\nNon-parametric\nneighbors: {15, 17, 19, 21},  weights: {uniform, distance}\n\n\n\n\nDuration Model Architecture\ndef get_model_definitions():\n    return {\n        'PenalizedLogNormal': (Pipeline([\n            ('log_transform', FunctionTransformer(\n                func=lambda x: np.log1p(np.clip(x, 1e-10, None)),\n                inverse_func=lambda x: np.expm1(x)\n            )),\n            ('ridge', Ridge())\n        ]), {\n            'model__ridge__alpha': np.logspace(0, 2, 20),\n            'select_features__k': np.arange(10, 55, 5),\n        }),\n    }\nThe duration models use log-normal distribution to handle right-skewed data, as shown in the distribution analysis:\n\n\n\nLog-normal Duration\n\n\n\n\nOccupancy Model Architecture\nThe occupancy models use a custom wrapper for count-based predictions:\nclass RoundedRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"Ensures integer predictions for occupancy modeling.\"\"\"\n    def __init__(self, estimator):\n        self.estimator = estimator\n        \n    def predict(self, X):\n        y_pred = self.estimator_.predict(X)\n        y_pred_rounded = np.round(y_pred).astype(int)\n        return np.maximum(y_pred_rounded, 0)  # Ensure non-negative\nThis wrapper enables count-based modeling through both the commonly shared Ridge, Lasso, PenalizedSplines, and KNN algorithms, as well as specialized distributions:\ndef get_model_definitions():\n    return {\n        'PenalizedPoisson': (RoundedRegressor(Pipeline([\n            ('log_link', FunctionTransformer(\n                func=lambda x: np.log(np.clip(x, 1e-10, None)),\n                inverse_func=lambda x: np.exp(np.clip(x, -10, 10))\n            )),\n            ('ridge', Ridge())\n        ])), {\n            'model__estimator__ridge__alpha': np.logspace(0, 2, 20),\n            'select_features__k': np.arange(70, 100, 10),\n        }),\n        'PenalizedWeibull': (RoundedRegressor(Pipeline([\n            ('weibull_link', FunctionTransformer(\n                func=lambda x: np.log(-np.log(1 - np.clip(x/(x.max()+1), 1e-10, 1-1e-10))),\n                inverse_func=lambda x: (1 - np.exp(-np.exp(np.clip(x, -10, 10)))) * (x.max() + 1)\n            )),\n            ('ridge', Ridge())\n        ])), {\n            'model__estimator__ridge__alpha': np.logspace(0, 2, 20),\n            'select_features__k': np.arange(70, 100, 10),\n        })\n    }\nThe occupancy data follows a Poisson (based on it being a count-based variable) distribution. We also found that a Weibull distribution could provide a better fit:\n\n\n\nPoisson Occupancy\n\n\n\n\n\nPipeline Architecture\nThe framework contains three preprocessing pipelines:\ndef get_pipeline_definitions():\n    return {\n        'vanilla': lambda model: Pipeline([\n            ('scaler', 'passthrough'), \n            ('model', model)\n        ]),\n        'interact_select': lambda model: Pipeline([\n            ('scaler', 'passthrough'), \n            ('interactions', PolynomialFeatures(\n                degree=2, \n                interaction_only=True, \n                include_bias=False\n            )),\n            ('select_features', SelectKBest(\n                score_func=f_regression, \n                k=100\n            )),\n            ('model', model)\n        ]),\n        'pca_lda': lambda model: Pipeline([\n            ('scaler', 'passthrough'), \n            ('feature_union', FeatureUnion([\n                ('pca', PCA(n_components=0.95)),\n                ('lda', LinearDiscriminantAnalysis(n_components=10)),\n            ])),\n            ('interactions', PolynomialFeatures(\n                degree=2, \n                interaction_only=True, \n                include_bias=False\n            )),\n            ('select_features', SelectKBest(\n                score_func=f_regression, \n                k=100\n            )),\n            ('model', model)\n        ])\n    }\n\nVanilla Pipeline\nThis configuration maintains feature interpretability while providing robust baseline performance through careful scaling of our engineered feature set.\n\n\nInteraction Network Pipeline\nThis interact_select pipeline implements a sparse interaction network, systematically capturing pairwise feature relationships while managing dimensionality through selective feature retention.\n\n\n\nInteraction Network\n\n\nThis approach was intended to function as a simplified mesh network, restricting connections to binary interactions without activation functions. The SelectKBest component manages dimensionality by identifying the most influential features and interactions.\n\n\nDimensionality Reduction Pipeline\nThis pipeline combines two complementary dimensionality reduction techniques before interaction modeling. We extract principal components that explain 95% of the variance (PCA) alongside 10 linear discriminant components (LDA), aiming to capture both the dominant patterns in feature variation and natural class separations in the data. These reduced-dimension components are then allowed to interact, with SelectKBest filtering the most predictive combinations."
  },
  {
    "objectID": "notebooks/report/models_overview.html#framework-integration",
    "href": "notebooks/report/models_overview.html#framework-integration",
    "title": "Models & Pipelines",
    "section": "Framework Integration",
    "text": "Framework Integration\nThe modeling framework described here serves as the foundation for our training and testing procedures. The model architectures process the engineered features from the previous chapter, while the pipeline configurations and cross-validation framework establish the structure for training optimization detailed in the next chapter.\nThe training chapter demonstrates how these components are orchestrated through MLflow experiment tracking and hyperparameter optimization."
  },
  {
    "objectID": "notebooks/report/eval.html#best-model-configurations",
    "href": "notebooks/report/eval.html#best-model-configurations",
    "title": "Evaluation",
    "section": "Best Model Configurations",
    "text": "Best Model Configurations\n\n\n\n\n\nDuration Model\n\n\n\nComponent\nConfiguration\n\n\n\n\nModel\nPenalizedSplines\n\n\nPipeline\nvanilla\n\n\nCV Method\nkfold\n\n\nRidge α\n14.38\n\n\nSpline knots\n15\n\n\nScaler\nRobustScaler\n\n\nRMSE\n59.47\n\n\nR²\n0.059\n\n\n\n\n\n\n\nOccupancy Model\n\n\n\nComponent\nConfiguration\n\n\n\n\nModel\nPenalizedSplines\n\n\nPipeline\nvanilla\n\n\nCV Method\nrolling\n\n\nRidge α\n29.76\n\n\nSpline knots\n15\n\n\nScaler\nRobustScaler\n\n\nRMSE\n3.64\n\n\nR²\n0.303\n\n\n\n\n\n\n\nFor predicting the Duration target variable, we implemented a Penalized Cubic Spline model with 15 knots. Despite extensive testing of various approaches, including feature interactions and compression techniques, the vanilla pipeline consistently outperformed more complex alternatives. Notably, temporal cross-validation showed no meaningful advantage over the random KFold method. After thorough hyperparameter optimization, we determined the optimal Ridge penalty to be 14.38.\nNote: While this model achieved the lowest RMSE among all tested approaches, its low R² value of 0.059 indicates a significant limitation in explaining the variance in the response variable.\nThe Occupancy prediction results proved more encouraging. Using another Penalized Cubic Spline model with 15 knots, we found that rolling cross-validation provided superior results compared to other methods. Through systematic tuning, we identified an optimal Ridge penalty of 29.76. The resulting R² value of 0.303, while modest, represents a meaningful improvement over the Duration model and suggests better capture of the underlying patterns in the data."
  },
  {
    "objectID": "notebooks/report/eval.html#model-diagnostics",
    "href": "notebooks/report/eval.html#model-diagnostics",
    "title": "Evaluation",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nDuration Model Performance\nThe diagnostic plots below visually evaluate the performance of the model in predicting the Duration response variable. As noted earlier, the model struggles to explain the variance in the response. These plots compare actual and predicted values and provide insights into the residuals.\nIn the first scatter plot, the model’s predictions are capped off just under roughly 150, leading to a horizontal cluster of points. This consistent underestimation of larger responses is further reflected in the residual histogram, which has an extended left tail, and the residual QQ plot, where many negative residuals deviate significantly below the normal reference line. Additionally, the residual plot reveals non-constant variance and a distinct pattern, indicating that the model fails to capture the true underlying structure of the data.\n\n\n\nDuration Model Diagnostics\n\n\n\nSummary: The duration prediction task proved particularly challenging, with models struggling to capture the full range of visit durations. The low R² value reflects the inherent complexity of predicting individual study session lengths. Our models consistently produced a compressed prediction range, systematically underestimating visits longer than 150 minutes while overestimating very short durations. This behavior suggests that additional features or alternative modeling approaches may be necessary to capture the full spectrum of study patterns.\n\n\n\nOccupancy Model Performance\nThe Occupancy model diagnostics reveal more promising results. The scatter plots exhibit a distinct grid-like pattern due to the integer nature of both the response variable and the model’s predictions. Areas where multiple points share the same value appear as darker shades of blue in the scatter plots.\nConsistent with the improved R² observed for this model, the diagnostic plots reflect a better overall fit compared to the Duration model. Although a slight pattern is still visible in the residual plot, it is evident that this model aligns more closely with the underlying data for this problem.\n\n\n\nOccupancy Model Diagnostics\n\n\n\nSummary: The occupancy prediction models demonstrated substantially better performance, particularly in capturing typical usage patterns. The models showed strongest accuracy in the modal range of 10-15 occupants, where most observations occur. While the overall distribution of predictions closely matched observed patterns, we observed minor discrepancies at the distribution tails, particularly during extremely busy or quiet periods. The superior R² value suggests that occupancy patterns follow more predictable trends than individual visit durations."
  },
  {
    "objectID": "notebooks/report/eval.html#distribution-analysis",
    "href": "notebooks/report/eval.html#distribution-analysis",
    "title": "Evaluation",
    "section": "Distribution Analysis",
    "text": "Distribution Analysis\n\nPredicted vs. Actual Distributions\nTo further evaluate the predictive performance of our models, we overlaid the distribution of predicted values onto histograms of the actual values. These plots provide a clearer depiction of how well our predictions align with the actual data. In both cases, the models appear to consistently overshoot the bulk of the observations. We suspect this overestimation is driven by the presence of extreme high values in the dataset, which the models struggled to accurately predict.\nThis challenge likely contributes to the relatively high RMSE for our Duration model. Large errors on these extreme values significantly inflate the RMSE, as it averages the squared differences across all predictions.\n\n\n\nDistribution Comparisons\n\n\nThe distribution comparisons above reveal distinct patterns for each response variable. For Duration (left panel), the predicted distribution (blue) exhibits notably less spread than the actual values (gray), with a pronounced peak near the mean. This compression of the predicted range manifests in consistent underestimation of extreme durations, particularly evident in the model’s inability to capture values beyond 150 minutes. These limitations align with our earlier observations of the model’s low R² (0.059) and elevated RMSE (59.47).\nThe Occupancy predictions (right panel) show more promising results. The predicted distribution more faithfully reproduces the actual data’s shape, particularly in the modal range of 10-15 occupants. However, some discrepancies persist at the extremes, with slight underestimation of low occupancy values and incomplete capture of the upper range. These characteristics explain the moderate R² value of 0.303, suggesting our model captures meaningful but incomplete patterns in the occupancy data.\n\n\nComparison of Prediction Methods\nThe visualization below compares two distinct approaches to occupancy prediction: direct modeling using Penalized Splines and imputation based on Duration predictions. The imputation method estimates occupancy as a derived variable from Duration predictions, while our primary model targets occupancy directly through Penalized Splines optimization.\nThis comparison reveals notable distributional differences. The direct modeling approach (shown in blue) demonstrates superior alignment with actual occupancy patterns, particularly in capturing the characteristic peak at 10-15 occupants. In contrast, the imputed predictions (shown in red) produce a more diffuse distribution with extended tails, suggesting systematic overestimation of extreme occupancy values. We attribute this overestimation to error propagation from the underlying Duration predictions.\nThese results underscore a critical methodological insight: while imputation offers a practical alternative when direct measurements are unavailable, it introduces additional uncertainty compared to models trained explicitly on the target variable. The superior performance of our direct Penalized Splines approach validates our decision to prioritize dedicated occupancy modeling.\n\n\n\nOccupancy Comparison Histogram"
  },
  {
    "objectID": "notebooks/report/eval.html#technical-challenges-and-insights",
    "href": "notebooks/report/eval.html#technical-challenges-and-insights",
    "title": "Evaluation",
    "section": "Technical Challenges and Insights",
    "text": "Technical Challenges and Insights\nOur modeling process revealed several key technical challenges that influenced our approach and results:\n\nDistribution Complexity\nThe underlying distributions of our target variables presented significant modeling challenges. Duration data exhibited strong right-skew characteristics, requiring careful consideration of transformation approaches. While log-normal transformations improved model training stability, they introduced complications in error interpretation on the original scale. The discrete nature of occupancy counts necessitated specialized handling, leading to our implementation of the RoundedRegressor wrapper to maintain prediction integrity.\n\n\nModel Architecture Considerations\nOur systematic evaluation of model architectures yielded an unexpected insight: the vanilla pipeline consistently outperformed more sophisticated approaches. This finding suggests that:\n\nThe relationship between our features and targets may be more direct than initially hypothesized\nThe additional complexity of interaction terms and dimensionality reduction might be introducing noise rather than capturing meaningful patterns\nThe superior performance of simpler architectures indicates that careful feature engineering may be more valuable than architectural sophistication\n\n\n\nTemporal Pattern Significance\nThe impact of temporal patterns emerged as a crucial factor, particularly in occupancy prediction. Rolling cross-validation consistently outperformed static approaches, suggesting that:\n\nRecent historical patterns carry strong predictive power\nThe relationship between features and occupancy evolves over time\nTraditional k-fold validation may underestimate model performance in practical applications\n\nThese insights have shaped our recommendations for future work and system deployment."
  },
  {
    "objectID": "notebooks/report/eval.html#conclusions",
    "href": "notebooks/report/eval.html#conclusions",
    "title": "Evaluation",
    "section": "Conclusions",
    "text": "Conclusions\n\nKey Findings\nOur analysis demonstrates that Penalized Cubic Spline models delivered the strongest performance for both Duration and Occupancy predictions. We are particularly satisfied with the Occupancy model’s capabilities and look forward to benchmarking it against our peers’ approaches.\n\n\nFuture Directions\nThis project’s scope was necessarily limited to the provided features. We believe incorporating external variables could substantially improve model performance. For instance, the timestamp data suggests an opportunity to integrate weather conditions, which likely influence Learning Commons usage patterns.\nAdditionally, while the models covered in this course offered valuable insights, alternative approaches such as tree-based methods or neural networks might better capture the complex non-linear relationships we observed. Given the clear temporal dependencies in our target variables, time series modeling presents another promising avenue for investigation."
  },
  {
    "objectID": "notebooks/report/predictions.html#configuration-management",
    "href": "notebooks/report/predictions.html#configuration-management",
    "title": "Predictions",
    "section": "Configuration Management",
    "text": "Configuration Management\nOur framework begins by loading the optimal model configurations identified during evaluation:\ndef load_model_config(model_type):\n    \"\"\"Load the best model configuration from JSON.\"\"\"\n    json_path = f\"results/best_models/{model_type}_best_model.json\"\n    with open(json_path, 'r') as f:\n        return json.load(f)\nThese configurations capture the winning combinations of model architecture, pipeline variant, and cross-validation strategy from our evaluation phase."
  },
  {
    "objectID": "notebooks/report/predictions.html#production-model-training",
    "href": "notebooks/report/predictions.html#production-model-training",
    "title": "Predictions",
    "section": "Production Model Training",
    "text": "Production Model Training\nWe implement the final training phase using the complete training dataset:\ndef train_model(model_type):\n    \"\"\"Train and save a model based on type (duration or occupancy).\"\"\"\n    config = load_model_config(model_type)\n    train_df = pd.read_csv(f'{project_root}/data/processed/train_engineered.csv')\n    X_train, y_train = prepare_data(train_df, target_var, features_to_drop)\n    \n    pipeline = create_penalized_splines_pipeline(config['parameters'], model_type)\n    pipeline.fit(X_train, y_train)\nThis approach maximizes model performance by utilizing all available training data while maintaining the validated configurations."
  },
  {
    "objectID": "notebooks/report/predictions.html#pipeline-implementation",
    "href": "notebooks/report/predictions.html#pipeline-implementation",
    "title": "Predictions",
    "section": "Pipeline Implementation",
    "text": "Pipeline Implementation\nWe reconstruct the optimal pipeline configurations for each prediction task:\ndef create_penalized_splines_pipeline(params, model_type='duration'):\n    \"\"\"Create pipeline with parameters from best model.\"\"\"\n    spline = SplineTransformer(degree=degree, n_knots=n_knots)\n    ridge = Ridge(alpha=alpha)\n    scaler = RobustScaler()\n    \n    base_pipeline = Pipeline([\n        ('scaler', scaler),\n        ('spline', spline),\n        ('ridge', ridge)\n    ])\nThe implementation maintains task-specific requirements: - Duration models use vanilla pipelines with optimized splines - Occupancy models incorporate RoundedRegressor for count constraints"
  },
  {
    "objectID": "notebooks/report/predictions.html#feature-alignment",
    "href": "notebooks/report/predictions.html#feature-alignment",
    "title": "Predictions",
    "section": "Feature Alignment",
    "text": "Feature Alignment\nWe ensure consistent feature processing between training and prediction:\ndef align_features_with_model(X_test, model):\n    \"\"\"Align test features with model's expected features.\"\"\"\n    model_features = model.named_steps['scaler'].feature_names_in_\n    \n    missing_features = set(model_features) - set(X_test.columns)\n    extra_features = set(X_test.columns) - set(model_features)\n    return X_test.reindex(columns=model_features)\nThis alignment prevents feature mismatch issues during prediction on LC_test."
  },
  {
    "objectID": "notebooks/report/predictions.html#prediction-generation",
    "href": "notebooks/report/predictions.html#prediction-generation",
    "title": "Predictions",
    "section": "Prediction Generation",
    "text": "Prediction Generation\nWe implement task-specific prediction protocols:\ndef main():\n    \"\"\"Generate predictions on test data.\"\"\"\n    X_test, _ = prepare_data(test_df, target, features_to_drop)\n    final_model = mlflow.sklearn.load_model(model_path)\n    X_test = align_features_with_model(X_test, final_model)\n    \n    # Task-specific prediction constraints\n    if model_type == 'duration':\n        predictions = np.maximum(final_model.predict(X_test), 1)\n    else:  # occupancy\n        predictions = np.round(np.maximum(\n            final_model.predict(X_test), 1\n        )).astype(int)\nEach task maintains its specific constraints: - Duration predictions enforce positive values - Occupancy predictions implement integer rounding"
  },
  {
    "objectID": "notebooks/report/predictions.html#results-management",
    "href": "notebooks/report/predictions.html#results-management",
    "title": "Predictions",
    "section": "Results Management",
    "text": "Results Management\nWe maintain systematic organization of prediction outputs:\ndef save_predictions(predictions, model_type):\n    \"\"\"Save predictions with appropriate formatting.\"\"\"\n    output_path = f'results/predictions/{model_type}_predictions.csv'\n    test_df[f'Predicted_{model_type.title()}'] = predictions\n    test_df.to_csv(output_path, index=False)\n    \n    # Generate summary statistics\n    summary_stats = {\n        'mean': predictions.mean(),\n        'std': predictions.std(),\n        'min': predictions.min(),\n        'max': predictions.max()\n    }\n    return summary_stats\nThis approach ensures: - Consistent output formatting - Validation through summary statistics - Clear organization of prediction files\nThese components work together to provide a robust prediction pipeline that maintains the methodological rigor established during model selection while maximizing prediction accuracy through full training data utilization."
  },
  {
    "objectID": "notebooks/report/appendix.html",
    "href": "notebooks/report/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Model Training\nThis chapter details our systematic approach to model training. Our training framework serves three key objectives:\nThe core training code is shared between two scripts: src/python/test_train/duration/TRAIN_duration.py and src/python/test_train/occupancy/TRAIN_occupancy.py. Like our testing scripts, these share approximately 95% of their code, differing primarily in:\nWe use MLflow to track experiments and maintain model versioning, enabling systematic comparison of different training configurations and reproducible results.\nThe complete implementation can be found in our GitHub repository.\nThis chapter details our systematic approach to model evaluation. Our testing framework serves three key objectives:\nThe core testing code is shared between two scripts: src/python/test_train/duration/TEST_duration.py and src/python/test_train/occupancy/TEST_occupancy.py. Like our training scripts, these share approximately 95% of their code, differing primarily in:\nWe use MLflow, an open-source platform for machine learning lifecycle management, to track our experiments and maintain model versioning. This enables reproducible testing and systematic comparison of different model configurations.\nThe complete implementation can be found in our GitHub repository.\nThis chapter details our systematic approach to model evaluation and comparison. Our evaluation framework serves three key objectives:\nThe core evaluation code is shared between two scripts: src/python/evaluation/model_evals.py and src/python/evaluation/best_model_params.py. These scripts work together to provide a complete evaluation pipeline for both duration and occupancy predictions."
  },
  {
    "objectID": "notebooks/report/appendix.html#technical-details",
    "href": "notebooks/report/appendix.html#technical-details",
    "title": "Appendix",
    "section": "Technical Details",
    "text": "Technical Details\nBelow are the technical details of our implementation from:\n\nTraining\nTesting\nEvaluation"
  },
  {
    "objectID": "notebooks/report/appendix.html#data-preparation-and-loading",
    "href": "notebooks/report/appendix.html#data-preparation-and-loading",
    "title": "Appendix",
    "section": "Data Preparation and Loading",
    "text": "Data Preparation and Loading\nOur first step involves loading the engineered features and preparing them for training:\ndef load_and_prepare_data(project_root: str):\n    \"\"\"Load and prepare the dataset.\"\"\"\n    df = pd.read_csv(f'{project_root}/data/processed/train_engineered.csv')\n    \n    target = 'Duration_In_Min'  # Changes to 'Occupancy' for occupancy training\n    features_to_drop = ['Student_IDs', 'Semester', 'Class_Standing', 'Major', \n                       'Expected_Graduation', ...]\n    \n    X, y = prepare_data(df, target, features_to_drop)\n    return train_test_split(X, y, test_size=0.2, shuffle=False)\nWe maintain chronological order by setting shuffle=False, as both duration and occupancy predictions exhibit temporal patterns. The features we drop are either identifiers or categorical variables already encoded during feature engineering."
  },
  {
    "objectID": "notebooks/report/appendix.html#core-training-architecture",
    "href": "notebooks/report/appendix.html#core-training-architecture",
    "title": "Appendix",
    "section": "Core Training Architecture",
    "text": "Core Training Architecture\nThe main training function orchestrates the entire process, setting up experiments and managing model iterations:\ndef train_models(X_train, y_train, X_test, y_test):\n    \"\"\"Main training function.\"\"\"\n    mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n    experiment_base = \"Duration_Pred\"\n    \n    # Get definitions\n    models = get_model_definitions()\n    pipelines = get_pipeline_definitions()\n    cv_methods = get_cv_methods(len(X_train))\n    scalers = [RobustScaler(), StandardScaler(), MinMaxScaler()]\nThis setup enables systematic experimentation across different model types, scaling approaches, and cross-validation strategies. The MLflow integration ensures reproducibility and experiment tracking."
  },
  {
    "objectID": "notebooks/report/appendix.html#single-model-training-implementation",
    "href": "notebooks/report/appendix.html#single-model-training-implementation",
    "title": "Appendix",
    "section": "Single Model Training Implementation",
    "text": "Single Model Training Implementation\nThe core training logic for individual models incorporates GridSearchCV with RMSE scoring:\ndef train_single_model(name, scale_type, cv_name, pipeline, params, cv, X_train, y_train):\n    \"\"\"Train a single model configuration.\"\"\"\n    rmse_scorer = make_scorer(\n        lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n        greater_is_better=False\n    )\n    \n    search = GridSearchCV(\n        pipeline, \n        params,\n        scoring=rmse_scorer,\n        cv=cv,\n        n_jobs=-1,\n        verbose=0,\n        error_score='raise'\n    )\nThe implementation leverages parallel processing through joblib’s ‘loky’ backend and includes comprehensive error handling. Each training iteration is tracked through MLflow:\n    with mlflow.start_run(run_name=model_name) as run:\n        mlflow.log_metric(\"rmse\", rmse_score)\n        mlflow.log_metric(\"rmse_std\", cv_std)\n        \n        # Log parameters\n        for param_name, param_value in search.best_params_.items():\n            mlflow.log_param(param_name, param_value)"
  },
  {
    "objectID": "notebooks/report/appendix.html#model-evaluation-framework",
    "href": "notebooks/report/appendix.html#model-evaluation-framework",
    "title": "Appendix",
    "section": "Model Evaluation Framework",
    "text": "Model Evaluation Framework\nThe evaluation system provides systematic assessment of trained models:\ndef evaluate_final_models(results, X_test, y_test):\n    \"\"\"Simple evaluation of models on test set without visualization.\"\"\"\n    final_results = []\n    \n    for result in results:\n        model_name = f\"{result['model']}_{result['pipeline_type']}_{result['cv_method']}\"\n        try:\n            model = mlflow.sklearn.load_model(f\"models:/{model_name}/latest\")\n            y_pred = model.predict(X_test)\n            \n            final_results.append({\n                'Model': result['model'],\n                'Pipeline': result['pipeline_type'],\n                'CV_Method': result['cv_method'],\n                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n                'R2': r2_score(y_test, y_pred)\n            })\nThis framework enables consistent evaluation across different model configurations while maintaining detailed performance metrics."
  },
  {
    "objectID": "notebooks/report/appendix.html#execution-flow",
    "href": "notebooks/report/appendix.html#execution-flow",
    "title": "Appendix",
    "section": "Execution Flow",
    "text": "Execution Flow\nThe main execution flow ties all components together:\ndef main():\n    # Load and prepare data\n    X_train, X_test, y_train, y_test = load_and_prepare_data(project_root)\n    \n    # Train models\n    results = train_models(X_train, y_train, X_test, y_test)\n    \n    # Save basic results\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(f'{project_root}/results/duration/training_results.csv', index=False)\n    \n    # Basic evaluation on test set\n    final_results = evaluate_final_models(results, X_test, y_test)\n    final_df = pd.DataFrame(final_results)\n    final_df.to_csv(f'{project_root}/results/duration/test_evaluation.csv', index=False)\nThis structured approach ensures reproducible training runs while maintaining comprehensive result logging and evaluation."
  },
  {
    "objectID": "notebooks/report/appendix.html#resource-management",
    "href": "notebooks/report/appendix.html#resource-management",
    "title": "Appendix",
    "section": "Resource Management",
    "text": "Resource Management\nThe implementation includes several key resource management features:\n\nParallel Processing: Utilizes joblib’s parallel backend for efficient computation\nMemory Management: Implements garbage collection after model training\nError Recovery: Includes comprehensive exception handling throughout the pipeline\n\nThese components work together to provide robust and efficient model training capabilities while maintaining consistent evaluation and deployment readiness."
  },
  {
    "objectID": "notebooks/report/appendix.html#data-preparation",
    "href": "notebooks/report/appendix.html#data-preparation",
    "title": "Appendix",
    "section": "Data Preparation",
    "text": "Data Preparation\nOur testing framework uses the same data split that was established during training to ensure consistent evaluation. We maintain the original 80/20 split of our dataset, where the 20% holdout set was never seen during model training or validation:\ndf = pd.read_csv(f'{project_root}/data/processed/train_engineered.csv')\n\ntarget = 'Duration_In_Min'  # Changes to 'Occupancy' for occupancy testing\nfeatures_to_drop = ['Student_IDs', 'Semester', 'Class_Standing', 'Major', \n                   'Expected_Graduation', ...]\n\nX, y = prepare_data(df, target, features_to_drop)\n\n# Using same holdout set from training\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    shuffle=False,  # Maintains chronological order\n    random_state=3  # Same seed used in training\n)\nBy using the same holdout set and random seed as training, we ensure:\n\nNo data leakage between training and testing\nFair comparison across all model variants\nConsistent evaluation of temporal patterns\nReliable assessment of model generalization\n\nThe framework tests models trained with three distinct cross-validation strategies:\n\nK-Fold (kfold): Provides baseline performance through random splits, particularly suitable for duration prediction\nRolling Window (rolling): Uses fixed-size moving windows to capture recent temporal dependencies, optimal for occupancy prediction\nExpanding Window (expanding): Implements growing windows that accumulate historical data, useful for long-term trends\n\nThese strategies are systematically evaluated in our testing loop:\npipeline_types = ['vanilla', 'interact_select', 'pca_lda']\ncv_methods = ['kfold', 'rolling', 'expanding']\n\nfor pipeline_type in pipeline_types:\n    for cv_name in cv_methods:\n        full_model_name = f\"{model_name}_{pipeline_type}_{cv_name}\"\nThis comprehensive approach allows us to assess how different cross-validation strategies affect model performance across our various pipeline configurations."
  },
  {
    "objectID": "notebooks/report/appendix.html#mlflow-configuration",
    "href": "notebooks/report/appendix.html#mlflow-configuration",
    "title": "Appendix",
    "section": "MLflow Configuration",
    "text": "MLflow Configuration\nThe testing framework relies heavily on MLflow for model management and experiment tracking:\nmlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n\ndef check_mlflow_connection():\n    try:\n        client = mlflow.tracking.MlflowClient()\n        client.search_experiments()\n        return True\n    except Exception as e:\n        print(f\"Error connecting to MLflow server: {e}\")\n        print(\"Please ensure MLflow server is running...\")\n        return False\nThis setup includes robust error handling for MLflow connectivity, ensuring the testing pipeline fails gracefully if the MLflow server is unavailable."
  },
  {
    "objectID": "notebooks/report/appendix.html#model-testing-implementation",
    "href": "notebooks/report/appendix.html#model-testing-implementation",
    "title": "Appendix",
    "section": "Model Testing Implementation",
    "text": "Model Testing Implementation\nThe core testing loop systematically evaluates each trained model variant:\nfor experiment in experiments:\n    model_name = experiment.name.split('/')[-1]\n    print(f\"\\nTesting models for {model_name}...\")\n    \n    # Pipeline types we used in training\n    pipeline_types = ['vanilla', 'interact_select', 'pca_lda']\n    cv_methods = ['kfold', 'rolling', 'expanding']\n    \n    for pipeline_type in pipeline_types:\n        for cv_name in cv_methods:\n            full_model_name = f\"{model_name}_{pipeline_type}_{cv_name}\"\n            try:\n                model = mlflow.sklearn.load_model(f\"models:/{full_model_name}/latest\")\n                y_pred = model.predict(X_test)\n                metrics = calculate_metrics(y_test, y_pred)\nThe implementation tests each combination of model type, pipeline configuration, and cross-validation method used during training."
  },
  {
    "objectID": "notebooks/report/appendix.html#performance-visualization",
    "href": "notebooks/report/appendix.html#performance-visualization",
    "title": "Appendix",
    "section": "Performance Visualization",
    "text": "Performance Visualization\nFor each model variant, we generate comprehensive visualization artifacts:\n# Create prediction analysis plots\nfig = plot_prediction_analysis(y_test, y_pred, full_model_name)\nplt.savefig(f'{project_root}/results/duration/prediction_analysis_{full_model_name}.png')\nplt.close()\n\n# Create feature importance biplot for the best model\nif metrics['RMSE'] &lt; best_model_rmse:\n    best_model_rmse = metrics['RMSE']\n    best_model_predictions = y_pred\n    plot_feature_importance_biplot(\n        X_test, y_test, y_pred, \n        X_test.columns,\n        f'{project_root}/results/duration'  # Changes for occupancy\n    )\nThese visualizations include prediction analysis plots and feature importance biplots, particularly for the best-performing models."
  },
  {
    "objectID": "notebooks/report/appendix.html#results-management",
    "href": "notebooks/report/appendix.html#results-management",
    "title": "Appendix",
    "section": "Results Management",
    "text": "Results Management\nThe framework maintains systematic records of test results:\n# If we have results for this experiment, keep only top 3 based on RMSE\nif experiment_results:\n    top_3_results = sorted(experiment_results, key=lambda x: x['RMSE'])[:3]\n    test_results.extend(top_3_results)\n\n# Create results DataFrame\nresults_df = pd.DataFrame(test_results)\nprint(\"\\nTop 3 Models per Experiment:\")\nprint(results_df.sort_values('RMSE'))\n\n# Save results and create visualization artifacts\nsave_visualization_results(results_df, project_root)\nThis approach ensures we maintain records of the best-performing models while generating comprehensive visualization artifacts for analysis."
  },
  {
    "objectID": "notebooks/report/appendix.html#resource-management-1",
    "href": "notebooks/report/appendix.html#resource-management-1",
    "title": "Appendix",
    "section": "Resource Management",
    "text": "Resource Management\nThe testing framework includes several key features for robust execution:\n\nError Handling: Comprehensive try-except blocks around model loading and prediction\nResource Cleanup: Systematic closure of visualization artifacts\nDirectory Management: Automatic creation of results directories\nProgress Logging: Detailed console output during testing\n\nThese components work together to provide reliable model evaluation while maintaining clear performance records and visualization artifacts."
  },
  {
    "objectID": "notebooks/report/appendix.html#performance-analysis-pipeline",
    "href": "notebooks/report/appendix.html#performance-analysis-pipeline",
    "title": "Appendix",
    "section": "Performance Analysis Pipeline",
    "text": "Performance Analysis Pipeline\nOur framework begins by aggregating performance metrics across different model configurations:\ndef load_and_analyze(filepath, dataset_name):\n    \"\"\"Hierarchical model performance analysis.\"\"\"\n    df = pd.read_csv(filepath)\n    \n    # Cross-validation strategy analysis\n    cv_groups = df.groupby('CV_Method')[['RMSE', 'R2']].agg(['mean', 'std'])\n    cv_groups = cv_groups.sort_values(('RMSE', 'mean'))\n    \n    # Pipeline architecture analysis\n    pipeline_groups = df.groupby('Pipeline')[['RMSE', 'R2']].agg(['mean', 'std'])\n    pipeline_groups = pipeline_groups.sort_values(('RMSE', 'mean'))\n    \n    # Model type analysis\n    model_groups = df.groupby('Model')[['RMSE', 'R2']].agg(['mean', 'std'])\n    model_groups = model_groups.sort_values(('RMSE', 'mean'))\nThis hierarchical analysis enables us to understand performance patterns across different aspects of our modeling approach."
  },
  {
    "objectID": "notebooks/report/appendix.html#best-model-identification",
    "href": "notebooks/report/appendix.html#best-model-identification",
    "title": "Appendix",
    "section": "Best Model Identification",
    "text": "Best Model Identification\nThe framework systematically identifies and extracts the optimal model configurations:\ndef get_best_model_params(eval_path, experiment_base):\n    \"\"\"Extract optimal model configuration.\"\"\"\n    # Identify best performer\n    df = pd.read_csv(eval_path)\n    best_row = df.loc[df['RMSE'].idxmin()]\n    \n    # Retrieve detailed configuration\n    model_name = f\"{best_row['Model']}_{best_row['Pipeline']}_{best_row['CV_Method']}\"\n    client = MlflowClient()\n    experiment = client.get_experiment_by_name(f\"{experiment_base}/{best_row['Model']}\")\nThis process ensures we capture not just the best performance metrics, but the complete configuration that achieved them."
  },
  {
    "objectID": "notebooks/report/appendix.html#results-documentation",
    "href": "notebooks/report/appendix.html#results-documentation",
    "title": "Appendix",
    "section": "Results Documentation",
    "text": "Results Documentation\nThe framework implements systematic result formatting and storage:\ndef save_and_print_results(results, dataset_type):\n    \"\"\"Format and persist evaluation results.\"\"\"\n    print(f\"\\n====== Best Model for {dataset_type.title()} Prediction ======\")\n    print(f\"Model: {results['model']}\")\n    print(f\"Pipeline: {results['pipeline']}\")\n    print(f\"CV Method: {results['cv_method']}\")\n    print(f\"RMSE: {results['rmse']:.4f}\")\n    print(f\"R2: {results['r2']:.4f}\")\n    \n    # Save configuration\n    output_file = os.path.join(output_dir, f\"{dataset_type}_best_model.json\")\n    with open(output_file, 'w') as f:\n        json.dump(results, f, indent=2)\nThis approach ensures consistent documentation of results across both prediction tasks."
  },
  {
    "objectID": "notebooks/report/appendix.html#evaluation-pipeline",
    "href": "notebooks/report/appendix.html#evaluation-pipeline",
    "title": "Appendix",
    "section": "Evaluation Pipeline",
    "text": "Evaluation Pipeline\nThe main evaluation pipeline processes both prediction tasks:\ndef main():\n    \"\"\"Execute comprehensive evaluation pipeline.\"\"\"\n    # Process occupancy results\n    occupancy_eval = \"results/occupancy/test_evaluation.csv\"\n    if os.path.exists(occupancy_eval):\n        occ_results = get_best_model_params(occupancy_eval, \"Occupancy_Pred\")\n        save_and_print_results(occ_results, \"occupancy\")\n    \n    # Process duration results\n    duration_eval = \"results/duration/test_evaluation.csv\"\n    if os.path.exists(duration_eval):\n        dur_results = get_best_model_params(duration_eval, \"Duration_Pred\")\n        save_and_print_results(dur_results, \"duration\")"
  },
  {
    "objectID": "notebooks/report/appendix.html#framework-components",
    "href": "notebooks/report/appendix.html#framework-components",
    "title": "Appendix",
    "section": "Framework Components",
    "text": "Framework Components\nThe implementation includes several key features for comprehensive evaluation:\n\nAnalysis Systems\n\nPerformance metric aggregation\nCross-model comparison\nStatistical analysis\n\nResult Processing\n\nParameter extraction\nConfiguration logging\nPerformance ranking\n\nOutput Generation\n\nResult formatting\nMetric visualization\nConfiguration persistence\n\n\nThese components work together to provide systematic performance analysis and model selection capabilities, ensuring we can confidently identify and document our best performing models."
  }
]