---
title: "Jason's Work"
author: "Jason Turk"
date: "2024-11-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r}
LC_engineered <- read.csv("https://raw.githubusercontent.com/adabwana/f24-m7550-final-project/refs/heads/jason-dev/data/LC_engineered.csv?token=GHSAT0AAAAAAC2XQKR2RGQOS7LE6XJ6BB7SZ2CSRLQ")
View(LC_engineered)
```

``` {r}
#Occupancy check
LC <- LC_engineered
#library(stringr)
#LCord <- LC[str_order(LC$Student_IDs),] ~Useful for checking indices of original data set's first 6 observations
LC$Occupancy[c(788, 93, 279, 5708, 458, 519)]
```

``` {r}
#LIBRARIES GO HERE
library(caret)
library(gam)

#Note: Run 'Compiled Dataset Cleaning' first to generate 'traindata' and 'testdata'
```

``` {r}
#POSSIBLE CENTER/SCALING OF NUMERICAL COVARIATES

TCH <- scale(traindata$Term_Credit_Hours, center = TRUE, scale = TRUE)
TGPA <- scale(traindata$Term_GPA, center = TRUE, scale = TRUE)
TCHE <- scale(traindata$Total_Credit_Hours_Earned, center = TRUE, scale = TRUE)
CGPA <- scale(traindata$Change_in_GPA, center = TRUE, scale = TRUE)

trainscale <- traindata
trainscale <- cbind(traindata, DIM, TCH, TGPA, TCHE, CGPA)
```



``` {r}
#LINEAR MODEL

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVMSE1 <- c()
for (i in 1:10) {
  CV1 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"), method = "lm")
  fitted1 <- predict(CV1, newdata = traindata[testInd[[i]],])
  CVMSE1[i] <- sum((fitted1 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE1 <- sum(CVMSE1)/10
RMSE1 <- sqrt(sum(CVMSE1)/10)
RMSE1
sqrt(var(CVMSE1))
```



``` {r}
#LINEAR MODEL WITH SEQUENTIAL SELECTION

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVMSE2 <- c()
for (i in 1:10) {
  CV2 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"), method = "leapSeq")
  fitted2 <- predict(CV2, newdata = traindata[testInd[[i]],])
  CVMSE2[i] <- sum((fitted2 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE2 <- sum(CVMSE2)/10
RMSE2 <- sqrt(sum(CVMSE2)/10)
RMSE2
sqrt(var(CVMSE2))

coef(CV2$finalModel, as.double(CV2$bestTune)) #See which variables were kept in
```



``` {r}
#LINEAR MODEL WITH REDUCED PARAMETERS (BASED ON SEQUENTIAL SELECTION)

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVMSE3 <- c()
for (i in 1:10) {
  CV3 <- train(Duration_In_Min ~ Day_of_Week + Major_Indicated + MATH
               + Evening, data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"), method = "lm")
  fitted3 <- predict(CV3, newdata = traindata[testInd[[i]],])
  CVMSE3[i] <- sum((fitted3 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE3 <- sum(CVMSE3)/10
RMSE3 <- sqrt(sum(CVMSE3)/10)
RMSE3
sqrt(var(CVMSE3))

summary(CV3)
```



``` {r}
#FORWARD SELECTION

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVMSE4 <- c()
for (i in 1:10) {
  CV4 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"), method = "leapForward")
  fitted4 <- predict(CV4, newdata = traindata[testInd[[i]],])
  CVMSE4[i] <- sum((fitted4 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE4 <- sum(CVMSE4)/10
RMSE4 <- sqrt(sum(CVMSE4)/10)
RMSE4
sqrt(var(CVMSE4))

coef(CV4$finalModel, as.double(CV4$bestTune)) #See which variables were kept in
```



``` {r}
#BACKWARD SELECTION

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVMSE5 <- c()
for (i in 1:10) {
  CV5 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"), method = "leapBackward")
  fitted5 <- predict(CV5, newdata = traindata[testInd[[i]],])
  CVMSE5[i] <- sum((fitted5 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE5 <- sum(CVMSE5)/10
RMSE5 <- sqrt(sum(CVMSE5)/10)
RMSE5
sqrt(var(CVMSE5))

coef(CV5$finalModel, as.double(CV5$bestTune)) #See which variables were kept in
```



``` {r}
#ELASTIC NET

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVtune6 <- train(Duration_In_Min ~ ., data = traindata,
                 preProc = c("center", "scale"), method = "glmnet",
                 tuneGrid = expand.grid(alpha = seq(0,1,by = 0.1),
                                        lambda = 10^seq(-4,4,by = 0.5)),
                 trControl=trainControl(method = 'cv',number = 10))

CVMSE6 <- c()
for (i in 1:10) {
  CV6 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"),  method = "glmnet",
               tuneGrid = CVtune6$bestTune)
  fitted6 <- predict(CV6, newdata = traindata[testInd[[i]],])
  CVMSE6[i] <- sum((fitted6 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE6 <- sum(CVMSE6)/10
RMSE6 <- sqrt(sum(CVMSE6)/10)
RMSE6
sqrt(var(CVMSE6))

CV6$bestTune #Alpha = 0.1 (very close to ridge), lambda = 0.0316
```



``` {r}
#RIDGE REGRESSION

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVtune7 <- train(Duration_In_Min ~ ., data = traindata,
                 preProc = c("center", "scale"), method = "glmnet",
                 tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-4,4,by = 0.5)),
                 trControl=trainControl(method = 'cv',number = 10))

CVMSE7 <- c()
for (i in 1:10) {
  CV7 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"),  method = "glmnet",
               tuneGrid = CVtune7$bestTune)
  fitted7 <- predict(CV5, newdata = traindata[testInd[[i]],])
  CVMSE7[i] <- sum((fitted7 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE7 <- sum(CVMSE7)/10
RMSE7 <- sqrt(sum(CVMSE7)/10)
RMSE7
sqrt(var(CVMSE7))

CV7$bestTune #Lambda = 0.316
```



``` {r}
#LASSO

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVtune8 <- train(Duration_In_Min ~ ., data = traindata,
                 preProc = c("center", "scale"), method = "glmnet",
                 tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(-4,4,by = 0.5)),
                 trControl=trainControl(method = 'cv',number = 10))

CVMSE8 <- c()
for (i in 1:10) {
  CV8 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"),  method = "glmnet",
               tuneGrid = CVtune8$bestTune)
  fitted8 <- predict(CV8, newdata = traindata[testInd[[i]],])
  CVMSE8[i] <- sum((fitted8 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE8 <- sum(CVMSE8)/10
RMSE8 <- sqrt(sum(CVMSE8)/10)
RMSE8
sqrt(var(CVMSE8))

CV8$bestTune #Lambda = 0.316
```



``` {r}
#PCR

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVtune9 <- train(Duration_In_Min ~ ., data = traindata,
                 preProc = c("center", "scale"), method = "pcr",
                 tuneLength = 16, trControl=trainControl(method = 'cv',number = 10))

CVMSE9 <- c()
for (i in 1:10) {
  CV9 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
               preProc = c("center", "scale"),  method = "pcr",
               tuneGrid = CVtune9$bestTune)
  fitted9 <- predict(CV9, newdata = traindata[testInd[[i]],])
  CVMSE9[i] <- sum((fitted9 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE9 <- sum(CVMSE9)/10
RMSE9 <- sqrt(sum(CVMSE9)/10)
RMSE9
sqrt(var(CVMSE9))

CV9$bestTune #15 predictors used
```



``` {r}
#PLS

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVtune10 <- train(Duration_In_Min ~ ., data = traindata,
                  preProc = c("center", "scale"), method = "pls",
                  tuneLength = 17, trControl=trainControl(method = 'cv',number = 10))

CVMSE10 <- c()
for (i in 1:10) {
  CV10 <- train(Duration_In_Min ~ ., data = traindata[-testInd[[i]],],
                preProc = c("center", "scale"),  method = "pls",
                tuneGrid = CVtune10$bestTune)
  fitted10 <- predict(CV10, newdata = traindata[testInd[[i]],])
  CVMSE10[i] <- sum((fitted10 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE10 <- sum(CVMSE10)/10
RMSE10 <- sqrt(sum(CVMSE10)/10)
RMSE10
sqrt(var(CVMSE10))

CV10$bestTune #12 predictors used
```



``` {r}
#NATURAL SPLINES (FOR QUANTITATIVE PREDICTORS)

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVMSE11 <- c()
for (i in 1:10) {
  CV11 <- gam(Duration_In_Min ~  Semester + Gender + Day_of_Week + Course_Level +
              Expected_Graduation_Yr + Underclassman + Major_Indicated + MATH +
              STAT + Exams_Approaching + Afternoon + Evening +
              ns(Term_Credit_Hours, df = 2) + ns(Term_GPA, df = 2) +
              ns(Total_Credit_Hours_Earned, df = 2) + ns(Change_in_GPA, df = 2),
              data = traindata[-testInd[[i]],])
  fitted11 <- predict(CV11, newdata = traindata[testInd[[i]],])
  CVMSE11[i] <- sum((fitted11 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE11 <- sum(CVMSE11)/10
RMSE11 <- sqrt(sum(CVMSE11)/10)
RMSE11
sqrt(var(CVMSE11))
```



``` {r}
#SMOOTHING SPLINES (FOR QUANTITATIVE PREDICTORS)

set.seed(7550)
testInd <- createFolds(traindata$Duration_In_Min, k = 10)

CVMSE12 <- c()
for (i in 1:10) {
  CV12 <- gam(Duration_In_Min ~  Semester + Gender + Day_of_Week + Course_Level +
              Expected_Graduation_Yr + Underclassman + Major_Indicated + MATH +
              STAT + Exams_Approaching + Afternoon + Evening +
              s(Term_Credit_Hours, df = 2) + s(Term_GPA, df = 2) +
              s(Total_Credit_Hours_Earned, df = 2) + s(Change_in_GPA, df = 2),
              data = traindata[-testInd[[i]],])
  fitted12 <- predict(CV12, newdata = traindata[testInd[[i]],])
  CVMSE12[i] <- sum((fitted12 - traindata$Duration_In_Min[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE12 <- sum(CVMSE12)/10
RMSE12 <- sqrt(sum(CVMSE12)/10)
RMSE12
sqrt(var(CVMSE12))
```



# PROBLEM 2: PREDICTING OCCUPANY
``` {r}
#LINEAR MODEL

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVMSE1b <- c()
for (i in 1:10) {
  CV1b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
                preProc = c("center", "scale"), method = "lm")
  fitted1b <- round(predict(CV1b, newdata = traindata2[testInd[[i]],]))
  CVMSE1b[i] <- sum((fitted1b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE1b <- sum(CVMSE1b)/10
RMSE1b <- sqrt(sum(CVMSE1b)/10)
RMSE1b
sqrt(var(CVMSE1b))
```



``` {r}
#LINEAR MODEL WITH SEQUENTIAL SELECTION

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVMSE2b <- c()
for (i in 1:10) {
  CV2b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
                preProc = c("center", "scale"), method = "leapSeq")
  fitted2b <- round(predict(CV2b, newdata = traindata2[testInd[[i]],]))
  CVMSE2b[i] <- sum((fitted2b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE2b <- sum(CVMSE2b)/10
RMSE2b <- sqrt(sum(CVMSE2b)/10)
RMSE2b
sqrt(var(CVMSE2b))

coef(CV2b$finalModel, as.double(CV2b$bestTune)) #See which variables were kept in
```



``` {r}
#FORWARD SELECTION

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVMSE4b <- c()
for (i in 1:10) {
  CV4b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
                preProc = c("center", "scale"), method = "leapForward")
  fitted4b <- predict(CV4b, newdata = traindata2[testInd[[i]],])
  CVMSE4b[i] <- sum((fitted4b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE4b <- sum(CVMSE4b)/10
RMSE4b <- sqrt(sum(CVMSE4b)/10)
RMSE4b
sqrt(var(CVMSE4b))

coef(CV4b$finalModel, as.double(CV4b$bestTune)) #See which variables were kept in
```



``` {r}
#BACKWARD SELECTION

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVMSE5b <- c()
for (i in 1:10) {
  CV5b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
                preProc = c("center", "scale"), method = "leapBackward")
  fitted5b <- predict(CV5b, newdata = traindata2[testInd[[i]],])
  CVMSE5b[i] <- sum((fitted5b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE5b <- sum(CVMSE5b)/10
RMSE5b <- sqrt(sum(CVMSE5b)/10)
RMSE5b
sqrt(var(CVMSE5b))

coef(CV5b$finalModel, as.double(CV5b$bestTune)) #See which variables were kept in
```



``` {r}
#LINEAR MODEL WITH REDUCED PARAMETERS (BASED ON FORWARD/BACKWARD SELECTION)

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVMSE3b <- c()
for (i in 1:10) {
  CV3b <- train(Occupancy ~ Semester + Day_of_Week + Afternoon +
                Evening, data = traindata2[-testInd[[i]],],
                preProc = c("center", "scale"), method = "lm")
  fitted3b <- round(predict(CV3b, newdata = traindata2[testInd[[i]],]))
  CVMSE3b[i] <- sum((fitted3b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE3b <- sum(CVMSE3b)/10
RMSE3b <- sqrt(sum(CVMSE3b)/10)
RMSE3b
sqrt(var(CVMSE3b))

summary(CV3b)
```



``` {r}
#ELASTIC NET / LASSO

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVtune6b <- train(Occupancy ~ ., data = traindata2,
                  preProc = c("center", "scale"), method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0,1,by = 0.1),
                                        lambda = 10^seq(-4,4,by = 0.5)),
                 trControl=trainControl(method = 'cv',number = 10))

CVMSE6b <- c()
for (i in 1:10) {
  CV6b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
               preProc = c("center", "scale"),  method = "glmnet",
               tuneGrid = CVtune6b$bestTune)
  fitted6b <- round(predict(CV6b, newdata = traindata2[testInd[[i]],]))
  CVMSE6b[i] <- sum((fitted6b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE6b <- sum(CVMSE6b)/10
RMSE6b <- sqrt(sum(CVMSE6b)/10)
RMSE6b
sqrt(var(CVMSE6b))

CV6b$bestTune #Alpha = 1 (pure lasso), lambda = 0.01
```



``` {r}
#RIDGE REGRESSION

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVtune7b <- train(Occupancy ~ ., data = traindata2,
                  preProc = c("center", "scale"), method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-4,4,by = 0.5)),
                 trControl=trainControl(method = 'cv',number = 10))

CVMSE7b <- c()
for (i in 1:10) {
  CV7b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
               preProc = c("center", "scale"),  method = "glmnet",
               tuneGrid = CVtune7b$bestTune)
  fitted7b <- round(predict(CV7b, newdata = traindata2[testInd[[i]],]))
  CVMSE7b[i] <- sum((fitted7b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE7b <- sum(CVMSE7b)/10
RMSE7b <- sqrt(sum(CVMSE7b)/10)
RMSE7b
sqrt(var(CVMSE7b))

CV7b$bestTune #Lambda = 0.1
```



``` {r}
#PCR

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVtune9b <- train(Occupancy ~ ., data = traindata2,
                  preProc = c("center", "scale"), method = "pcr",
                  tuneLength = 16, trControl=trainControl(method = 'cv',number = 10))

CVMSE9b <- c()
for (i in 1:10) {
  CV9b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
               preProc = c("center", "scale"),  method = "pcr",
               tuneGrid = CVtune9b$bestTune)
  fitted9b <- round(predict(CV9b, newdata = traindata2[testInd[[i]],]))
  CVMSE9b[i] <- sum((fitted9b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE9b <- sum(CVMSE9b)/10
RMSE9b <- sqrt(sum(CVMSE9b)/10)
RMSE9b
sqrt(var(CVMSE9b))

CV9b$bestTune #All 16 predictors used!
```



``` {r}
#PLS

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVtune10b <- train(Occupancy ~ ., data = traindata2,
                   preProc = c("center", "scale"), method = "pls",
                   tuneLength = 16, trControl=trainControl(method = 'cv',number = 10))

CVMSE10b <- c()
for (i in 1:10) {
  CV10b <- train(Occupancy ~ ., data = traindata2[-testInd[[i]],],
                preProc = c("center", "scale"),  method = "pls",
                tuneGrid = CVtune10b$bestTune)
  fitted10b <- round(predict(CV10b, newdata = traindata2[testInd[[i]],]))
  CVMSE10b[i] <- sum((fitted10b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE10b <- sum(CVMSE10b)/10
RMSE10b <- sqrt(sum(CVMSE10b)/10)
RMSE10b
sqrt(var(CVMSE10b))

CV10b$bestTune #9 predictors used
```



``` {r}
#NATURAL SPLINES (FOR QUANTITATIVE PREDICTORS)

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVMSE11b <- c()
for (i in 1:10) {
  CV11b <- gam(Occupancy ~  Semester + Gender + Day_of_Week + Course_Level +
               Expected_Graduation_Yr + Underclassman + Major_Indicated + MATH +
               STAT + Exams_Approaching + Afternoon + Evening +
               ns(Term_Credit_Hours, df = 2) + ns(Term_GPA, df = 2) +
               ns(Total_Credit_Hours_Earned, df = 2) + ns(Change_in_GPA, df = 2),
               data = traindata2[-testInd[[i]],])
  fitted11b <- round(predict(CV11b, newdata = traindata2[testInd[[i]],]))
  CVMSE11b[i] <- sum((fitted11b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE11b <- sum(CVMSE11b)/10
RMSE11b <- sqrt(sum(CVMSE11b)/10)
RMSE11b
sqrt(var(CVMSE11b))
```



``` {r}
#SMOOTHING SPLINES (FOR QUANTITATIVE PREDICTORS)

set.seed(7550)
testInd <- createFolds(traindata2$Occupancy, k = 10)

CVMSE12b <- c()
for (i in 1:10) {
  CV12b <- gam(Occupancy ~  Semester + Gender + Day_of_Week + Course_Level +
               Expected_Graduation_Yr + Underclassman + Major_Indicated + MATH +
               STAT + Exams_Approaching + Afternoon + Evening +
               s(Term_Credit_Hours, df = 2) + s(Term_GPA, df = 2) +
               s(Total_Credit_Hours_Earned, df = 2) + s(Change_in_GPA, df = 2),
               data = traindata2[-testInd[[i]],])
  fitted12b <- round(predict(CV12b, newdata = traindata2[testInd[[i]],]))
  CVMSE12b[i] <- sum((fitted12b - traindata2$Occupancy[testInd[[i]]])^2)/
               length(testInd[[i]])
}

MSE12b <- sum(CVMSE12b)/10
RMSE12b <- sqrt(sum(CVMSE12b)/10)
RMSE12b
sqrt(var(CVMSE12b))
```



``` {r}
# POISSON REGRESSION


```


